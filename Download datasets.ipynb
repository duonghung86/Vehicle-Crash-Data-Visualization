{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page = requests.get(\"https://www.nhtsa.gov/node/97996/251\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all hyperlink in the website\n",
    "hpl= soup.find_all('a',href=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n",
      "https://www.nhtsa.gov/node/97996/117196\n",
      "2010\n",
      "https://www.nhtsa.gov/node/97996/117201\n",
      "2011\n",
      "https://www.nhtsa.gov/node/97996/117206\n",
      "2012\n",
      "https://www.nhtsa.gov/node/97996/117211\n",
      "2013\n",
      "https://www.nhtsa.gov/node/97996/117216\n",
      "2014\n",
      "https://www.nhtsa.gov/node/97996/117221\n",
      "2015\n",
      "https://www.nhtsa.gov/node/97996/117226\n",
      "2016\n",
      "https://www.nhtsa.gov/node/97996/117231\n",
      "2017\n",
      "https://www.nhtsa.gov/node/97996/117236\n",
      "2018\n",
      "https://www.nhtsa.gov/node/97996/176766\n"
     ]
    }
   ],
   "source": [
    "#Find all the link to each year\n",
    "dl={} # variable that contains link to each year\n",
    "for ele in hpl:\n",
    "    e_text=ele.get_text()\n",
    "    try:\n",
    "        year=int(e_text)\n",
    "        if (year>2008 and year <2019):\n",
    "            print(year)\n",
    "            print(ele['href'])\n",
    "            dl[year]=ele['href']\n",
    "    except: continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download zip file of each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n",
      "100% [..........................................................................] 9707266 / 9707266\n",
      "2010\n",
      "100% [........................................................................] 13810722 / 13810722\n",
      "2011\n",
      "100% [........................................................................] 14099035 / 14099035\n",
      "2012\n",
      "100% [........................................................................] 15029317 / 15029317\n",
      "2013\n",
      "100% [........................................................................] 25191966 / 25191966\n",
      "2014\n",
      "100% [........................................................................] 25401135 / 25401135\n",
      "2015\n",
      "100% [........................................................................] 28731973 / 28731973\n",
      "2016\n",
      "100% [........................................................................] 21821873 / 21821873\n",
      "2017\n",
      "100% [........................................................................] 22270170 / 22270170\n",
      "2018\n",
      "100% [........................................................................] 22070494 / 22070494\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "for year in dl.keys():\n",
    "    print(year)\n",
    "    # find and select the national link\n",
    "    page1 = requests.get(dl[year])\n",
    "    soup1 = BeautifulSoup(page1.content, 'html.parser')\n",
    "    hpl1= soup1.find_all('a',href=True)\n",
    "    sublink=''\n",
    "    for ele in hpl1:\n",
    "        if ele.get_text()=='National':\n",
    "            sublink=ele['href']\n",
    "            break\n",
    "    \n",
    "    #find and select the csv file\n",
    "    page1_1 = requests.get(sublink)\n",
    "    soup1_1 = BeautifulSoup(page1_1.content, 'html.parser')\n",
    "    hpl1_1= soup1_1.find_all('a',href=True)\n",
    "    \n",
    "    dll=''\n",
    "    for ele in hpl1_1:\n",
    "        if ele.get_text().endswith('SAS.zip'):\n",
    "            dll=ele['href']\n",
    "            break\n",
    "    \n",
    "    # Download file to the Data sets folder\n",
    "    wget.download(dll,out='./Datasets')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2009: 'FARS2009NationalSAS.zip',\n",
       " 2010: 'FARS2010NationalSAS.zip',\n",
       " 2011: 'FARS2011NationalSAS.zip',\n",
       " 2012: 'FARS2012NationalSAS.zip',\n",
       " 2013: 'FARS2013NationalSAS.zip',\n",
       " 2014: 'FARS2014NationalSAS.zip',\n",
       " 2015: 'FARS2015NationalSAS.zip',\n",
       " 2016: 'FARS2016NationalSAS.zip',\n",
       " 2017: 'FARS2017NationalSAS.zip',\n",
       " 2018: 'FARS2018NationalSAS.zip'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist=os.listdir('./Datasets')\n",
    "dict_file=dict(zip(dl.keys(),filelist))\n",
    "dict_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year,fname in dict_file.items():\n",
    "    path='./Datasets/'+fname\n",
    "    zip_ref=ZipFile(path,'r')\n",
    "    zip_ref.extractall('./Datasets/'+str(year))\n",
    "    zip_ref.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
